\section[IB]{Information Bottleneck} 



\begin{frame}
    \frametitle{The Information Bottleneck}
    Let $\mathbb{X}, \mathbb{Y}$ be random variables with a known joint probability distribution $P_{\mathbb{X}, \mathbb{Y}}$, $\mathcal{X}, \mathcal{Y}$ their domain and  $|\mathcal{X}| \in \mathbb{N}, |\mathcal{Y}| \in \mathbb{N}$. \newline

    Intuition: we want to encode a message $\mathbb{X}$ such that we keep as much information about $\mathbb{Y}$ as possible, while compressing $\mathbb{X}$ as much as possible. \newline

    We achieve this by finding a soft partioning of $\mathbb{X}$ defined by a mapping $P_{\hat{X}|\mathbb{X}}$ such that $I(\hat{X},\mathbb{X})$ is minimized while $I(\hat{X},\mathbb{Y})$ is maximized. \newline

    The solution is the minima of the functional:
    \begin{equation}
        P_{\hat{X}|\mathbb{X}} = argmin_{p(\hat{X}|\mathbb{X})} 
        I(\hat{X},\mathbb{X}) - \beta I(\hat{X}, \mathbb{Y})
    \end{equation}
    This was introduced by \cite{tishby_information_2000}.


    
% - Rate distortion theory gives us a limit on how much compression is possible:
%     - rate distortion definition
%     - Blahut-Arimoto algorithm can be used to find optimal mapping
%     - is there any constraint on d in the original rate distortion theory? "Unlike the case of rate distortion theory, here the constraint on the meaningful information is nonlinear in the desired mapping p(Ëœx|x) and this is a much
%     harder variational problem." - Why?
% - now set d := I(x', x) => You get the IB problem and Blahut-Arimoto becomes The IB iterative algorithm
\end{frame}


\begin{frame}
    \frametitle{The Information Bottleneck Iterative Algorithm}
    Similar to the Blahut-Arimoto Algorithm (\cite{blahut_computation_1972} \cite{arimoto_algorithm_1972}) for finding an optimal $P_{\hat{X}|\mathbb{X}}$, there exists an iterative algorithm that converges to the optimal solution for the IB problem:

    \begin{align*}
        p_t(\hat{x}|x) &= \frac{p_t(\hat{x})}{Z_t(x, \beta)} exp(- \beta d(x, \hat{x}) ) \\
        p_{t+1}(\hat{x}) &= \sum_{x} p_t(\hat{x}|x) P_{\mathbb{X}}(x) \\
        p_{t+1}(y|\hat{x}) &= \frac{p_t(y, x')}{p_{t+1}(x')} = \frac{\sum_{x} P_{\mathbb{X},\mathbb{Y}}(x, y) p_t(\hat{x}|x)}{p_{t+1}(x')} 
    \end{align*} 

    Where $d(x, \hat{x}) = D_{KL}(P_{\mathbb{Y}|\mathbb{X}}|| P_{\mathbb{Y}|\hat{X}})$ \footnote{
        In equation (31) of \cite{tishby_information_2000}, the update rule is stated as $p_{t+1}(y|\hat{x}) = \sum_y P_{\mathbb{Y}|\mathbb{X}}(y|x) p_t(x|\hat{x})$, which cannot be correct since it does not depend on $y$ of the lefthand side of the equation.
    }
    
\end{frame}
