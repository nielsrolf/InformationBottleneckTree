
@article{tishby_information_2000,
	title = {The information bottleneck method},
	url = {http://arxiv.org/abs/physics/0004057},
	abstract = {We define the relevant information in a signal \$x{\textbackslash}in X\$ as being the information that this signal provides about another signal \$y{\textbackslash}in {\textbackslash}Y\$. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal \$x\$ requires more than just predicting \$y\$, it also requires specifying which features of \${\textbackslash}X\$ play a role in the prediction. We formalize this problem as that of finding a short code for \${\textbackslash}X\$ that preserves the maximum information about \${\textbackslash}Y\$. That is, we squeeze the information that \${\textbackslash}X\$ provides about \${\textbackslash}Y\$ through a `bottleneck' formed by a limited set of codewords \${\textbackslash}{tX}\$. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure \$d(x,{\textbackslash}x)\$ emerges from the joint statistics of \${\textbackslash}X\$ and \${\textbackslash}Y\$. This approach yields an exact set of self consistent equations for the coding rules \$X {\textbackslash}to {\textbackslash}{tX}\$ and \${\textbackslash}{tX} {\textbackslash}to {\textbackslash}Y\$. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
	journaltitle = {{arXiv}:physics/0004057},
	author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
	urldate = {2020-07-25},
	date = {2000-04-24},
	eprinttype = {arxiv},
	eprint = {physics/0004057},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Physics - Data Analysis, Statistics and Probability},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/JIYN3793/Tishby et al. - 2000 - The information bottleneck method.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/RNCHU5HY/0004057.html:text/html}
}

@article{arimoto_algorithm_1972,
	title = {An algorithm for computing the capacity of arbitrary discrete memoryless channels},
	volume = {18},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1054753/},
	doi = {10.1109/TIT.1972.1054753},
	pages = {14--20},
	number = {1},
	journaltitle = {{IEEE} Transactions on Information Theory},
	shortjournal = {{IEEE} Trans. Inform. Theory},
	author = {Arimoto, S.},
	urldate = {2020-07-25},
	date = {1972-01},
	langid = {english},
	file = {Arimoto - 1972 - An algorithm for computing the capacity of arbitra.pdf:/Users/nielswarncke/Zotero/storage/UXPVY99M/Arimoto - 1972 - An algorithm for computing the capacity of arbitra.pdf:application/pdf}
}

@article{blahut_computation_1972,
	title = {Computation of channel capacity and rate-distortion functions},
	volume = {18},
	issn = {1557-9654},
	doi = {10.1109/TIT.1972.1054855},
	abstract = {By defining mutual information as a maximum over an appropriate space, channel capacities can be defined as double maxima and rate-distortion functions as double minima. This approach yields valuable new insights regarding the computation of channel capacities and rate-distortion functions. In particular, it suggests a simple algorithm for computing channel capacity that consists of a mapping from the set of channel input probability vectors into itself such that the sequence of probability vectors generated by successive applications of the mapping converges to the vector that achieves the capacity of the given channel. Analogous algorithms then are provided for computing rate-distortion functions and constrained channel capacities. The algorithms apply both to discrete and to continuous alphabet channels or sources. In addition, a formalization of the theory of channel capacity in the presence of constraints is included. Among the examples is the calculation of close upper and lower bounds to the rate-distortion function of a binary symmetric Markov source.},
	pages = {460--473},
	number = {4},
	journaltitle = {{IEEE} Transactions on Information Theory},
	author = {Blahut, R.},
	date = {1972-07},
	note = {Conference Name: {IEEE} Transactions on Information Theory},
	keywords = {Mutual information, Rate-distortion theory},
	file = {Eingereichte Version:/Users/nielswarncke/Zotero/storage/27GIHBV4/Blahut - 1972 - Computation of channel capacity and rate-distortio.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/nielswarncke/Zotero/storage/6XQ7BBVB/1054855.html:text/html}
}

@incollection{rokach_decision_2005,
	location = {Boston, {MA}},
	title = {Decision Trees},
	isbn = {978-0-387-25465-4},
	url = {https://doi.org/10.1007/0-387-25465-X_9},
	abstract = {Decision Trees are considered to be one of the most popular approaches for representing classifiers. Researchers from various disciplines such as statistics, machine learning, pattern recognition, and Data Mining have dealt with the issue of growing a decision tree from available data. This paper presents an updated survey of current methods for constructing decision tree classifiers in a top-down manner. The chapter suggests a unified algorithmic framework for presenting these algorithms and describes various splitting criteria and pruning methodologies.},
	pages = {165--192},
	booktitle = {Data Mining and Knowledge Discovery Handbook},
	publisher = {Springer {US}},
	author = {Rokach, Lior and Maimon, Oded},
	editor = {Maimon, Oded and Rokach, Lior},
	date = {2005},
	doi = {10.1007/0-387-25465-X_9}
}

@book{mitchell_machine_1997,
	location = {{USA}},
	edition = {1},
	title = {Machine Learning},
	isbn = {978-0-07-042807-2},
	abstract = {This exciting addition to the {McGraw}-Hill Series in Computer Science focuses on the concepts and techniques that contribute to the rapidly changing field of machine learning--including probability and statistics, artificial intelligence, and neural networks--unifying them all in a logical and coherent manner. Machine Learning serves as a useful reference tool for software developers and researchers, as well as an outstanding text for college students. Table of contents Chapter 1. Introduction Chapter 2. Concept Learning and the General-to-Specific Ordering Chapter 3. Decision Tree Learning Chapter 4. Artificial Neural Networks Chapter 5. Evaluating Hypotheses Chapter 6. Bayesian Learning Chapter 7. Computational Learning Theory Chapter 8. Instance-Based Learning Chapter 9. Inductive Logic Programming Chapter 10. Analytical Learning Chapter 11. Combining Inductive and Analytical Learning Chapter 12. Reinforcement Learning.},
	pagetotal = {432},
	publisher = {{McGraw}-Hill, Inc.},
	author = {Mitchell, Thomas M.},
	date = {1997}
}

@article{achille_information_2017,
	title = {Information Dropout: Learning Optimal Representations Through Noisy Computation},
	url = {http://arxiv.org/abs/1611.01353},
	shorttitle = {Information Dropout},
	abstract = {The cross-entropy loss commonly used in deep learning is closely related to the defining properties of optimal representations, but does not enforce some of the key properties. We show that this can be solved by adding a regularization term, which is in turn related to injecting multiplicative noise in the activations of a Deep Neural Network, a special case of which is the common practice of dropout. We show that our regularized loss function can be efficiently minimized using Information Dropout, a generalization of dropout rooted in information theoretic principles that automatically adapts to the data and can better exploit architectures of limited capacity. When the task is the reconstruction of the input, we show that our loss function yields a Variational Autoencoder as a special case, thus providing a link between representation learning, information theory and variational inference. Finally, we prove that we can promote the creation of disentangled representations simply by enforcing a factorized prior, a fact that has been observed empirically in recent work. Our experiments validate the theoretical intuitions behind our method, and we find that information dropout achieves a comparable or better generalization performance than binary dropout, especially on smaller models, since it can automatically adapt the noise to the structure of the network, as well as to the test sample.},
	journaltitle = {{arXiv}:1611.01353 [cs, stat]},
	author = {Achille, Alessandro and Soatto, Stefano},
	urldate = {2020-07-26},
	date = {2017-02-12},
	eprinttype = {arxiv},
	eprint = {1611.01353},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/AM7BT7WI/Achille und Soatto - 2017 - Information Dropout Learning Optimal Representati.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/ML4AEK6L/1611.html:text/html}
}

@article{belghazi_mine_2018,
	title = {{MINE}: Mutual Information Neural Estimation},
	url = {http://arxiv.org/abs/1801.04062},
	shorttitle = {{MINE}},
	abstract = {We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator ({MINE}) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which {MINE} can be used to minimize or maximize mutual information. We apply {MINE} to improve adversarially trained generative models. We also use {MINE} to implement Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.},
	journaltitle = {{arXiv}:1801.04062 [cs, stat]},
	author = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeswar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, R. Devon},
	urldate = {2020-07-26},
	date = {2018-06-07},
	eprinttype = {arxiv},
	eprint = {1801.04062},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/nielswarncke/Zotero/storage/3E8T2BVH/Belghazi et al. - 2018 - MINE Mutual Information Neural Estimation.pdf:application/pdf;arXiv.org Snapshot:/Users/nielswarncke/Zotero/storage/SN5SR385/1801.html:text/html}
}